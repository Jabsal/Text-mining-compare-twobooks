{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sim_py_textJEWK.ipynb\n",
    "\n",
    "### Text mining: compare two books\n",
    "\n",
    "# Packages\n",
    "\n",
    "### Install packages\n",
    "\n",
    "\"codecs\" is for reading the text files, \n",
    "\"re\" (regular expretions) and \"collections\" for working with tokens,\n",
    "\"nltk\" (natural language toolkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install scipy\n",
    "!pip install sklearn\n",
    "!pip install nltk\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "import copy\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download stopwords\n",
    "\n",
    "### Some specialized functions from NLTK\n",
    "You can also download everything in NLTK with nltk.download(), but it will take time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the stopwords package from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "### Read data for Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open('JaneEyre.txt', \"r\", encoding=\"utf-8\") as f:\n",
    "    text_JE = f.read()\n",
    "with codecs.open('WutheringHeights.txt', \"r\", encoding=\"utf-8\") as f:\n",
    "    text_WH = f.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data\n",
    "Check for stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "esw = stopwords.words('english')\n",
    "esw.append(\"would\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter tokens (using regular expressions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_pattern = re.compile(\"^\\w+$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a token counter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text_counter(text):\n",
    "    tokens = WordPunctTokenizer().tokenize(PorterStemmer().stem(text))\n",
    "    tokens = list(map(lambda x: x.lower(), tokens))\n",
    "    tokens = [token for token in tokens if re.match(word_pattern, token) and token not in esw]\n",
    "    return collections.Counter(tokens), len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to calculate the absolute frequency of the most commen words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_df(counter, size):\n",
    "    abs_freq = np.array([el[1] for el in counter])\n",
    "    rel_freq = abs_freq / size\n",
    "    index = [el[0] for el in counter]\n",
    "    df = pd.DataFrame(data=np.array([abs_freq, rel_freq]).T, index=index, columns=[\"Absolute frequency\", \"Relative frequency\"])\n",
    "    df.index.name = \"Most common words\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "## Analyze individual texts\n",
    "\n",
    "Calculate the most common words of Jane Eyre and display the 15 most common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "je_counter, je_size = get_text_counter(text_JE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_df(je_counter.most_common(15), je_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the 1000 most common words of Jane Eyre to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "je_df = make_df(je_counter.most_common(1000), je_size)\n",
    "je_df.to_csv(\"JE2_1000.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the most common words of Withering Hights and display the 15 most common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wh_counter, wh_size = get_text_counter(text_WH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_df(wh_counter.most_common(15), wh_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the 1000 most common words of Withering Hights to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_df = make_df(wh_counter.most_common(1000), wh_size)\n",
    "wh_df.to_csv(\"WH2_1000.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare texts\n",
    "\n",
    "Find the most common words across the two documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_counter = wh_counter + je_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = make_df(wh_counter.most_common(1000), 1)\n",
    "most_common_words = all_df.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Create a data frame with the differences in word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_data = []\n",
    "for word in most_common_words:\n",
    "    je_c = je_counter.get(word, 0) / je_size\n",
    "    wh_c = wh_counter.get(word, 0) / wh_size\n",
    "    d = abs(je_c - wh_c)\n",
    "    df_data.append([je_c, wh_c, d])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diff_df = pd.DataFrame(data=df_data, index=most_common_words,\n",
    "                          columns=[\"JE relative frequency\", \"WH relative frequency\", \"Differences in relative frequency\"])\n",
    "diff_df.index.name = \"Most common words\"\n",
    "diff_df.sort_values(\"Differences in relative frequency\", ascending=False, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the most 20 distinctive words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the full list of distinctive words to a dist_JEWH.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_df.to_csv(\"dist_JEWH.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
